{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf4a4148-10b0-4358-9cda-a6755be6e4b9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Reading Data from Bronze layer"
    }
   },
   "outputs": [],
   "source": [
    "cards_data_df = spark.read.format(\"delta\").load(\"/mnt/adls21s/bronze/cards_data/\")\n",
    "display(cards_data_df)\n",
    "\n",
    "transactions_data_df = spark.read.format(\"delta\").load(\"/mnt/adls21s/bronze/transactions_data/\")\n",
    "transactions_data_df.show()\n",
    "\n",
    "users_data_df = spark.read.format(\"delta\").load(\"/mnt/adls21s/bronze/users_data/\")\n",
    "display(users_data_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06455c1b-20fc-4d9c-b8db-c6724d7f25e0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cleaning Data"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, col, count, when\n",
    "\n",
    "'''Remove special characters'''\n",
    "cards_data_df = cards_data_df.withColumn(\"credit_limit_in_$\", regexp_replace('credit_limit', '\\$', ''))\n",
    "cards_data_df = cards_data_df.drop(\"credit_limit\")\n",
    "transactions_data_df = transactions_data_df.withColumn(\"amount_in_$\", regexp_replace(\"amount\", \"\\$\", \"\"))\n",
    "transactions_data_df = transactions_data_df.drop(\"amount\")\n",
    "users_data_df = users_data_df.withColumn(\"per_capita_income_in_$\", regexp_replace(\"per_capita_income\", \"\\$\", \"\"))\n",
    "users_data_df = users_data_df.drop(\"per_capita_income\")\n",
    "users_data_df = users_data_df.withColumn(\"yearly_income_in_$\", regexp_replace(\"yearly_income\", \"\\$\", \"\"))\n",
    "users_data_df = users_data_df.drop(\"yearly_income\")\n",
    "users_data_df = users_data_df.withColumn(\"total_debt_in_$\", regexp_replace(\"total_debt\", \"\\$\", \"\"))\n",
    "users_data_df = users_data_df.drop(\"total_debt\")\n",
    "\n",
    "'''Changing NULL values in transactions_data[error] column to No Errors'''\n",
    "transactions_data_df = transactions_data_df.fillna({\"errors\": \"No Errors\"})\n",
    "\n",
    "'''Checking NULL values in each dataframe'''\n",
    "null_counts_cards_data_df = display(cards_data_df.select([count(when(col(c).isNull(), c)).alias(c) for c in cards_data_df.columns]))\n",
    "null_counts_transactions_data_df = display(transactions_data_df.select([count(when(col(c).isNull(), c)).alias(c) for c in transactions_data_df.columns]))\n",
    "null_counts_users_data_df = display(users_data_df.select([count(when(col(c).isNull(), c)).alias(c) for c in users_data_df.columns]))\n",
    "\n",
    "'''Replacing NULL values in zip where merchant_city is Not ONLINE but zip is NULL to ABROAD'''\n",
    "transactions_data_df = transactions_data_df.withColumn(\"zip\", when((col(\"merchant_city\") != \"ONLINE\")&col(\"zip\").isNull(), \"ABROAD\").otherwise(col(\"zip\")))\n",
    "\n",
    "'''Replacing NULL values in zip where merchant_city is ONLINE and merchant_state, zip are NULL to ONLINE'''\n",
    "transactions_data_df = transactions_data_df.fillna({\"zip\": \"ONLINE\", \"merchant_state\": \"ONLINE\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0b30c79-28f8-443d-b588-8594a4b939ce",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Data Masking PII"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "'''Masking card number and cvv columns'''\n",
    "def mask_card_number(card):\n",
    "    charlist = list(str(card))\n",
    "    charlist[1:-1] = \"X\"*len(charlist[1:-1])\n",
    "    return \"\".join(charlist)\n",
    "\n",
    "def mask_cvv(cvv):\n",
    "    charlist = list(str(cvv))\n",
    "    charlist[:] = \"X\"*len(charlist)\n",
    "    return \"\".join(charlist)\n",
    "\n",
    "mask_card_udf = udf(mask_card_number, StringType())\n",
    "mask_cvv_udf = udf(mask_cvv, StringType())\n",
    "\n",
    "cards_data_df_masked = cards_data_df.withColumn(\"masked_card_number\", mask_card_udf(cards_data_df[\"card_number\"])).withColumn(\"masked_cvv\", mask_cvv_udf(cards_data_df[\"cvv\"]))\n",
    "\n",
    "cards_data_df_masked.show()\n",
    "\n",
    "'''Masking Address, latitude and longitude columns'''\n",
    "def mask_address(address):\n",
    "    charlist = list(address)\n",
    "    charlist[1:-1] = \"X\"*len(charlist[1:-1])\n",
    "    return \"\".join(charlist)\n",
    "\n",
    "def mask_latitude(latitude):\n",
    "    charlist = list(str(latitude))\n",
    "    charlist[:] = \"X\"*len(charlist)\n",
    "    return \"\".join(charlist)\n",
    "\n",
    "def mask_longitude(longitude):\n",
    "    charlist = list(str(longitude))\n",
    "    charlist[:] = \"X\"*len(charlist)\n",
    "    return \"\".join(charlist)\n",
    "\n",
    "mask_address_udf = udf(mask_address, StringType())\n",
    "mask_latitude_udf = udf(mask_latitude, StringType())\n",
    "mask_longitude_udf = udf(mask_longitude, StringType())\n",
    "\n",
    "users_data_df_masked = users_data_df.withColumn(\"masked_address\", mask_address_udf(users_data_df[\"address\"])).withColumn(\"masked_latitude\", mask_latitude_udf(users_data_df[\"latitude\"])).withColumn(\"masked_longitude\", mask_longitude_udf(users_data_df[\"longitude\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec068042-f891-4fd2-8ab9-44c310b9e843",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Schema Refinement/Enforcing"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format, year, to_timestamp, month, to_date\n",
    "\n",
    "cards_data_df_cleaned = cards_data_df_masked.withColumn(\"id\", col(\"id\").cast(\"int\"))\\\n",
    "                                            .withColumn(\"client_id\", col(\"client_id\").cast(\"int\"))\\\n",
    "                                            .withColumn(\"card_brand\", col(\"card_brand\").cast(\"string\"))\\\n",
    "                                            .withColumn(\"card_type\", col(\"card_type\").cast(\"string\"))\\\n",
    "                                            .drop(\"card_number\")\\\n",
    "                                            .withColumn(\"expires\", date_format(\"expires\", \"MM/yyyy\"))\\\n",
    "                                            .drop(\"cvv\")\\\n",
    "                                            .withColumn(\"year_pin_last_changed\", year(to_date(col(\"year_pin_last_changed\"), 'yyyy')))\\\n",
    "                                            .withColumn(\"has_chip\", col(\"has_chip\").cast(\"string\"))\\\n",
    "                                            .withColumn(\"num_cards_issued\", col(\"num_cards_issued\").cast(\"int\"))\\\n",
    "                                            .withColumn(\"acct_open_date\", date_format(\"acct_open_date\", \"MM/yyyy\"))\\\n",
    "                                            .withColumn(\"card_on_dark_web\", col(\"card_on_dark_web\").cast(\"string\"))\\\n",
    "                                            .withColumn(\"credit_limit_in_$\", col(\"credit_limit_in_$\").cast(\"long\"))\\\n",
    "                                            .withColumn(\"masked_card_number\", col(\"masked_card_number\").cast(\"string\"))\\\n",
    "                                            .withColumn(\"masked_cvv\", col(\"masked_cvv\").cast(\"string\"))\\\n",
    "                                            .withColumn(\"ingested_at\", to_timestamp(\"ingested_at\", \"dd/MM/yyyy HH:mm:ss\"))\n",
    "\n",
    "users_data_df_cleaned = users_data_df_masked.withColumn(\"id\", col(\"id\").cast(\"int\"))\\\n",
    "                                            .withColumn(\"current_age\", col(\"current_age\").cast(\"int\"))\\\n",
    "                                            .withColumn(\"retirement_age\", col(\"retirement_age\").cast(\"int\"))\\\n",
    "                                            .withColumn(\"birth_year\", year(to_date(col(\"birth_year\"), 'yyyy')))\\\n",
    "                                            .withColumn(\"birth_month\", month(to_date(col(\"birth_month\"), 'MM')))\\\n",
    "                                            .withColumn(\"gender\", col(\"gender\").cast(\"string\"))\\\n",
    "                                            .drop(\"address\")\\\n",
    "                                            .drop(\"latitude\")\\\n",
    "                                            .drop(\"longitude\")\\\n",
    "                                            .withColumn(\"credit_score\", col(\"credit_score\").cast(\"int\"))\\\n",
    "                                            .withColumn(\"num_credit_cards\", col(\"num_credit_cards\").cast(\"int\"))\\\n",
    "                                            .withColumn(\"per_capita_income_in_$\", col(\"per_capita_income_in_$\").cast(\"long\"))\\\n",
    "                                            .withColumn(\"yearly_income_in_$\", col(\"yearly_income_in_$\").cast(\"long\"))\\\n",
    "                                            .withColumn(\"total_debt_in_$\", col(\"total_debt_in_$\").cast(\"long\"))\\\n",
    "                                            .withColumn(\"masked_address\", col(\"masked_address\").cast(\"string\"))\\\n",
    "                                            .withColumn(\"masked_latitude\", col(\"masked_latitude\").cast(\"string\"))\\\n",
    "                                            .withColumn(\"masked_longitude\", col(\"masked_longitude\").cast(\"string\"))\\\n",
    "                                            .withColumn(\"ingested_at\", to_timestamp(\"ingested_at\", \"dd/MM/yyyy HH:mm:ss\"))\n",
    "\n",
    "transactions_data_df_cleaned = transactions_data_df.withColumn(\"id\", col(\"id\").cast(\"int\"))\\\n",
    "                                                   .withColumn(\"client_id\", col(\"client_id\").cast(\"int\"))\\\n",
    "                                                   .withColumn(\"card_id\", col(\"card_id\").cast(\"int\"))\\\n",
    "                                                   .withColumn(\"use_chip\", col(\"use_chip\").cast(\"string\"))\\\n",
    "                                                   .withColumn(\"amount_in_$\", col(\"amount_in_$\").cast(\"long\"))\\\n",
    "                                                   .withColumn(\"merchant_id\", col(\"merchant_id\").cast(\"int\"))\\\n",
    "                                                   .withColumn(\"merchant_city\", col(\"merchant_city\").cast(\"string\"))\\\n",
    "                                                   .withColumn(\"merchant_state\", col(\"merchant_state\").cast(\"string\"))\\\n",
    "                                                   .withColumn(\"zip\", col(\"zip\").cast(\"string\"))\\\n",
    "                                                   .withColumn(\"mcc\", col(\"mcc\").cast(\"int\"))\\\n",
    "                                                   .withColumn(\"errors\", col(\"errors\").cast(\"string\"))\\\n",
    "                                                   .withColumn(\"ingested_at\", to_timestamp(\"ingested_at\", \"dd/MM/yyyy HH:mm:ss\"))\n",
    "\n",
    "cards_data_df_cleaned.printSchema()\n",
    "users_data_df_cleaned.printSchema()\n",
    "transactions_data_df_cleaned.printSchema()\n",
    "transactions_data_df_cleaned.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5b26491-94be-4bbf-94e3-80aedf34e126",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Writing Data to Silver"
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Path to Delta table\n",
    "users_data_delta_table_path = \"/mnt/adls21s/silver/users_data/\"\n",
    "cards_data_delta_table_path = \"/mnt/adls21s/silver/cards_data/\"\n",
    "transactions_data_delta_table_path = \"/mnt/adls21s/silver/transactions_data/\"\n",
    "\n",
    "# Create Delta table (if not already created)\n",
    "if not DeltaTable.isDeltaTable(spark, users_data_delta_table_path):\n",
    "    users_data_df_cleaned.write.format(\"delta\").mode(\"overwrite\").save(users_data_delta_table_path)\n",
    "    if not DeltaTable.isDeltaTable(spark,cards_data_delta_table_path):\n",
    "        cards_data_df_cleaned.write.format(\"delta\").mode(\"overwrite\").save(cards_data_delta_table_path)\n",
    "        if not DeltaTable.isDeltaTable(spark, transactions_data_delta_table_path):\n",
    "            transactions_data_df_cleaned.write.format(\"delta\").mode(\"overwrite\").save(transactions_data_delta_table_path)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "# Load the existing Delta table\n",
    "users_data_delta_table = DeltaTable.forPath(spark, (users_data_delta_table_path))\n",
    "cards_data_delta_table = DeltaTable.forPath(spark,(cards_data_delta_table_path))\n",
    "transactions_data_delta_table = DeltaTable.forPath(spark,(transactions_data_delta_table_path))\n",
    "\n",
    "# Define the condition for the merge: match based on a unique key\n",
    "merge_condition = \"t1.id = t2.id\"\n",
    "\n",
    "# Perform the merge (upsert operation)\n",
    "users_data_delta_table.alias(\"t1\").merge(\n",
    "    users_data_df_masked.alias(\"t2\"),\n",
    "    merge_condition\n",
    ").whenMatchedUpdateAll() \\\n",
    " .whenNotMatchedInsertAll() \\\n",
    " .execute()\n",
    "\n",
    "cards_data_delta_table.alias(\"t1\").merge(\n",
    "    cards_data_df_masked.alias(\"t2\"),\n",
    "    merge_condition\n",
    ").whenMatchedUpdateAll() \\\n",
    " .whenNotMatchedInsertAll() \\\n",
    " .execute()\n",
    "\n",
    "transactions_data_delta_table.alias(\"t1\").merge(\n",
    "    transactions_data_df_cleaned.alias(\"t2\"),\n",
    "    merge_condition\n",
    ").whenMatchedUpdateAll() \\\n",
    " .whenNotMatchedInsertAll() \\\n",
    " .execute()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
