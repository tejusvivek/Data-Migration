{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e28e548d-73fa-4497-b56f-24832283f021",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Service Principal Access to ADLS"
    }
   },
   "outputs": [],
   "source": [
    "dbutils.secrets.listScopes()\n",
    "dbutils.secrets.list(\"adls-key\")\n",
    "service_credential = dbutils.secrets.get(scope=\"adls-key\",key=\"migrationAppAuth\")\n",
    "application_id = \"5403bb6e-a9d0-44a6-a025-de1ebc2a7881\"\n",
    "directory_id = \"4249dcf4-f4a1-44f9-940d-14b50a777dd8\"\n",
    "\n",
    "spark.conf.set(\"fs.azure.account.auth.type.adls21s.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(\"fs.azure.account.oauth.provider.type.adls21s.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.id.adls21s.dfs.core.windows.net\", application_id)\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.secret.adls21s.dfs.core.windows.net\", service_credential)\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.endpoint.adls21s.dfs.core.windows.net\", \"https://login.microsoftonline.com/{}/oauth2/token\".format(directory_id))\n",
    "\n",
    "display(dbutils.fs.ls(\"abfss://raw@adls21s.dfs.core.windows.net\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65aa51bf-37a4-42f8-ade1-61358a93aab6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Mount All Containers"
    }
   },
   "outputs": [],
   "source": [
    "configs = {\"fs.azure.account.auth.type\": \"OAuth\",\n",
    "          \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "          \"fs.azure.account.oauth2.client.id\": application_id,\n",
    "          \"fs.azure.account.oauth2.client.secret\": service_credential,\n",
    "          \"fs.azure.account.oauth2.client.endpoint\": f\"https://login.microsoftonline.com/{directory_id}/oauth2/token\"}\n",
    "\n",
    "mountsDict = {\"/mnt/adls21s/raw/\" : \"abfss://raw@adls21s.dfs.core.windows.net/\",\n",
    "              \"/mnt/schema/\" : \"abfss://schema@adls21s.dfs.core.windows.net/\",\n",
    "              \"/mnt/adls21s/bronze/\" : \"abfss://bronze@adls21s.dfs.core.windows.net/\",\n",
    "              \"/mnt/adls21s/silver/\" : \"abfss://silver@adls21s.dfs.core.windows.net/\",\n",
    "              \"/mnt/adls21s/gold/\" : \"abfss://gold@adls21s.dfs.core.windows.net/\"}\n",
    "\n",
    "def checkIfMounted(mounts):\n",
    "    if(any(mount.mountPoint == \"/mnt/adls21s/raw/\" for mount in mounts )):\n",
    "        print(\"Mount point exists, Unmounting...\")\n",
    "        dbutils.fs.unmount(\"/mnt/adls21s/raw/\")\n",
    "        print(\"Unmounted Successfully\")\n",
    "        dbutils.fs.mount(source = \"abfss://raw@adls21s.dfs.core.windows.net/\", mount_point = \"/mnt/adls21s/raw/\", extra_configs = configs)\n",
    "        print(\"Mounted Successfully\")\n",
    "        display(dbutils.fs.mounts())\n",
    "    else:\n",
    "        print(\"Mount point does not exist\")\n",
    "        dbutils.fs.mount(source = \"abfss://raw@adls21s.dfs.core.windows.net/\", mount_point = \"/mnt/adls21s/raw/\", extra_configs = configs)\n",
    "        print(\"Mounted Successfully\")\n",
    "\n",
    "for mountPoint in mountsDict:\n",
    "    try:\n",
    "        dbutils.fs.mount(source = mountsDict[mountPoint], mount_point = mountPoint, extra_configs = configs)\n",
    "        print('No existing mount point found, Mounting {}'.format(mountPoint))\n",
    "    except Exception as e:\n",
    "        #checkIfMounted(mountsList)\n",
    "        print(\"Mount point {} exists, skipping...\".format(mountPoint))\n",
    "display(dbutils.fs.mounts())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5be73d97-cd04-46ce-a0bc-4116f993cc96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, DateType, DoubleType, ShortType,TimestampNTZType\n",
    "\n",
    "cardsDataSchema = StructType([StructField(\"id\", IntegerType(), True), \n",
    "                              StructField(\"client_id\", IntegerType(), True), \n",
    "                              StructField(\"card_brand\", StringType(), True), \n",
    "                              StructField(\"card_type\", StringType(), True), \n",
    "                              StructField(\"card_number\", LongType(), True), \n",
    "                              StructField(\"expires\", StringType(), True), \n",
    "                              StructField(\"cvv\", ShortType(), True), \n",
    "                              StructField(\"has_chip\", StringType(), True), \n",
    "                              StructField(\"num_cards_issued\", ShortType(), True), \n",
    "                              StructField(\"credit_limit\", StringType(), True), \n",
    "                              StructField(\"acct_open_date\", StringType(), True), \n",
    "                              StructField(\"year_pin_last_changed\", IntegerType(), True), \n",
    "                              StructField(\"card_on_dark_web\", StringType(), True)])\n",
    "\n",
    "\n",
    "raw_cards_data_df = spark.read.format(\"csv\").option(\"header\", \"true\").schema(cardsDataSchema)\\\n",
    "    .load(\"/mnt/adls21s/raw/cards_data.csv\")\n",
    "display(raw_cards_data_df)\n",
    "\n",
    "tranactionsDataSchema = StructType([StructField(\"id\", IntegerType(), True),\n",
    "                                    StructField(\"date\", TimestampNTZType(), True),\n",
    "                                    StructField(\"client_id\", IntegerType(), True),\n",
    "                                    StructField(\"card_id\", IntegerType(), True),\n",
    "                                    StructField(\"amount\", StringType(), True),\n",
    "                                    StructField(\"use_chip\", StringType(), True),\n",
    "                                    StructField(\"merchant_id\", IntegerType(), True),\n",
    "                                    StructField(\"merchant_city\", StringType(), True),\n",
    "                                    StructField(\"merchant_state\", StringType(), True),\n",
    "                                    StructField(\"zip\", StringType(), True),\n",
    "                                    StructField(\"mcc\", IntegerType(), True),\n",
    "                                    StructField(\"errors\", StringType(), True)])\n",
    "\n",
    "raw_transactions_data_df = spark.read.format(\"csv\").option(\"header\", \"true\").schema(tranactionsDataSchema)\\\n",
    "    .load(\"/mnt/adls21s/raw/transactions_data.csv\")\n",
    "raw_transactions_data_df.show()\n",
    "\n",
    "usersDataSchema = StructType([StructField(\"id\", IntegerType(), True),\n",
    "                              StructField(\"current_age\", IntegerType(), True),\n",
    "                              StructField(\"retirement_age\", IntegerType(), True),\n",
    "                              StructField(\"birth_year\", IntegerType(), True),\n",
    "                              StructField(\"birth_month\", IntegerType(), True),\n",
    "                              StructField(\"gender\", StringType(), True),\n",
    "                              StructField(\"address\", StringType(), True),\n",
    "                              StructField(\"latitude\", DoubleType(), True),\n",
    "                              StructField(\"longitude\", DoubleType(), True),\n",
    "                              StructField(\"per_capita_income\", StringType(), True),\n",
    "                              StructField(\"yearly_income\", StringType(), True),\n",
    "                              StructField(\"total_debt\", StringType(), True),\n",
    "                              StructField(\"credit_score\", IntegerType(), True),\n",
    "                              StructField(\"num_credit_cards\", IntegerType(), True)])\n",
    "                              \n",
    "raw_users_data_df = spark.read.format(\"csv\").option(\"header\", \"true\").schema(usersDataSchema)\\\n",
    "    .load(\"/mnt/adls21s/raw/users_data.csv\")\n",
    "display(raw_users_data_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bba6f00-6449-47e6-97a9-2dbd65f5dc46",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Adding timestamp and  Removing NULL values from dataframe"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, regexp_replace, col, count, when, length, year\n",
    "\n",
    "'''Add timestamp column to dataframe'''\n",
    "raw_cards_data_df = raw_cards_data_df.withColumn(\"ingested_at\", current_timestamp())\n",
    "raw_transactions_data_df = raw_transactions_data_df.withColumn(\"ingested_at\", current_timestamp())\n",
    "raw_users_data_df = raw_users_data_df.withColumn(\"ingested_at\", current_timestamp())\n",
    "\n",
    "'''Checking for invalid values'''\n",
    "raw_users_data_df.filter(\"current_age < 0\" or \"retirement_age < 0\" or \"birth_year < 0\" or \"birth_month < 0\" or \"credit_score < 0\" or \"current_age>100\" or \"credit_score > 900\").show()\n",
    "\n",
    "raw_cards_data_df.filter(length(\"card_number\") != 16).show()\n",
    "\n",
    "raw_users_data_df.filter((year(\"ingested_at\") - \"birth_year\") > 115).show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cdac6232-269c-485e-82be-c8dba9425513",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "'''Remove special characters'''\n",
    "cards_data_df = raw_cards_data_df.withColumn(\"credit_limit_in_$\", regexp_replace('credit_limit', '\\$', ''))\n",
    "cards_data_df = cards_data_df.drop(\"credit_limit\")\n",
    "transactions_data_df = raw_transactions_data_df.withColumn(\"amount_in_$\", regexp_replace(\"amount\", \"\\$\", \"\"))\n",
    "transactions_data_df = transactions_data_df.drop(\"amount\")\n",
    "users_data_df = raw_users_data_df.withColumn(\"per_capita_income_in_$\", regexp_replace(\"per_capita_income\", \"\\$\", \"\"))\n",
    "users_data_df = users_data_df.drop(\"per_capita_income\")\n",
    "users_data_df = users_data_df.withColumn(\"yearly_income_in_$\", regexp_replace(\"yearly_income\", \"\\$\", \"\"))\n",
    "users_data_df = users_data_df.drop(\"yearly_income\")\n",
    "users_data_df = users_data_df.withColumn(\"total_debt_in_$\", regexp_replace(\"total_debt\", \"\\$\", \"\"))\n",
    "users_data_df = users_data_df.drop(\"total_debt\")\n",
    "\n",
    "'''Changing NULL values in transactions_data[error] column to No Errors'''\n",
    "transactions_data_df = transactions_data_df.fillna({\"errors\": \"No Errors\"})\n",
    "\n",
    "'''Checking NULL values in each dataframe'''\n",
    "null_counts_cards_data_df = display(cards_data_df.select([count(when(col(c).isNull(), c)).alias(c) for c in cards_data_df.columns]))\n",
    "null_counts_transactions_data_df = display(transactions_data_df.select([count(when(col(c).isNull(), c)).alias(c) for c in transactions_data_df.columns]))\n",
    "null_counts_users_data_df = display(users_data_df.select([count(when(col(c).isNull(), c)).alias(c) for c in users_data_df.columns]))\n",
    "\n",
    "'''Replacing NULL values in zip where merchant_city is Not ONLINE but zip is NULL to ABROAD'''\n",
    "transactions_data_df = transactions_data_df.withColumn(\"zip\", when((col(\"merchant_city\") != \"ONLINE\")&col(\"zip\").isNull(), \"ABROAD\").otherwise(col(\"zip\")))\n",
    "\n",
    "'''Replacing NULL values in zip where merchant_city is ONLINE and merchant_state, zip are NULL to ONLINE'''\n",
    "transactions_data_df = transactions_data_df.fillna({\"zip\": \"ONLINE\", \"merchant_state\": \"ONLINE\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3eba8161-cca1-4678-a814-0cacc57244cf",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Data Masking PII and Encryption"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "'''Masking card number and cvv columns'''\n",
    "def mask_card_number(card):\n",
    "    charlist = list(str(card))\n",
    "    charlist[1:-1] = \"X\"*len(charlist[1:-1])\n",
    "    return \"\".join(charlist)\n",
    "\n",
    "def mask_cvv(cvv):\n",
    "    charlist = list(str(cvv))\n",
    "    charlist[:] = \"X\"*len(charlist)\n",
    "    return \"\".join(charlist)\n",
    "\n",
    "mask_card_udf = udf(mask_card_number, StringType())\n",
    "mask_cvv_udf = udf(mask_cvv, StringType())\n",
    "\n",
    "cards_data_df_masked = cards_data_df.withColumn(\"masked_card_number\", mask_card_udf(cards_data_df[\"card_number\"])).withColumn(\"masked_cvv\", mask_cvv_udf(cards_data_df[\"cvv\"]))\n",
    "\n",
    "cards_data_df_masked.show()\n",
    "\n",
    "'''Masking Address, latitude and longitude columns'''\n",
    "def mask_address(address):\n",
    "    charlist = list(address)\n",
    "    charlist[1:-1] = \"X\"*len(charlist[1:-1])\n",
    "    return \"\".join(charlist)\n",
    "\n",
    "def mask_latitude(latitude):\n",
    "    charlist = list(str(latitude))\n",
    "    charlist[:] = \"X\"*len(charlist)\n",
    "    return \"\".join(charlist)\n",
    "\n",
    "def mask_longitude(longitude):\n",
    "    charlist = list(str(longitude))\n",
    "    charlist[:] = \"X\"*len(charlist)\n",
    "    return \"\".join(charlist)\n",
    "\n",
    "mask_address_udf = udf(mask_address, StringType())\n",
    "mask_latitude_udf = udf(mask_latitude, StringType())\n",
    "mask_longitude_udf = udf(mask_longitude, StringType())\n",
    "\n",
    "users_data_df_masked = users_data_df.withColumn(\"masked_address\", mask_address_udf(users_data_df[\"address\"])).withColumn(\"masked_latitude\", mask_latitude_udf(users_data_df[\"latitude\"])).withColumn(\"masked_longitude\", mask_longitude_udf(users_data_df[\"longitude\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "619edddf-48cb-431e-b3f8-c505a149bfb0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Writing data to Bronze stage"
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Path to Delta table\n",
    "users_data_delta_table_path = \"/mnt/adls21s/bronze/users_data/\"\n",
    "cards_data_delta_table_path = \"/mnt/adls21s/bronze/cards_data/\"\n",
    "transactions_data_delta_table_path = \"/mnt/adls21s/bronze/transactions_data/\"\n",
    "\n",
    "# Create Delta table (if not already created)\n",
    "if not DeltaTable.isDeltaTable(spark, users_data_delta_table_path):\n",
    "    raw_users_data_df.write.format(\"delta\").mode(\"overwrite\").save(users_data_delta_table_path)\n",
    "    if not DeltaTable.isDeltaTable(spark,cards_data_delta_table_path):\n",
    "        raw_cards_data_df.write.format(\"delta\").mode(\"overwrite\").save(cards_data_delta_table_path)\n",
    "        if not DeltaTable.isDeltaTable(spark, transactions_data_delta_table_path):\n",
    "            raw_transactions_data_df.write.format(\"delta\").mode(\"overwrite\").save(transactions_data_delta_table_path)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "# Load the existing Delta table\n",
    "users_data_delta_table = DeltaTable.forPath(spark, (users_data_delta_table_path))\n",
    "cards_data_delta_table = DeltaTable.forPath(spark,(cards_data_delta_table_path))\n",
    "transactions_data_delta_table = DeltaTable.forPath(spark,(transactions_data_delta_table_path))\n",
    "\n",
    "# Define the condition for the merge: match based on a unique key\n",
    "merge_condition = \"t1.id = t2.id\"\n",
    "\n",
    "# Perform the merge (upsert operation)\n",
    "users_data_delta_table.alias(\"t1\").merge(\n",
    "    raw_users_data_df.alias(\"t2\"),\n",
    "    merge_condition\n",
    ").whenMatchedUpdateAll() \\\n",
    " .whenNotMatchedInsertAll() \\\n",
    " .execute()\n",
    "\n",
    "cards_data_delta_table.alias(\"t1\").merge(\n",
    "    raw_cards_data_df.alias(\"t2\"),\n",
    "    merge_condition\n",
    ").whenMatchedUpdateAll() \\\n",
    " .whenNotMatchedInsertAll() \\\n",
    " .execute()\n",
    "\n",
    "transactions_data_delta_table.alias(\"t1\").merge(\n",
    "    raw_transactions_data_df.alias(\"t2\"),\n",
    "    merge_condition\n",
    ").whenMatchedUpdateAll() \\\n",
    " .whenNotMatchedInsertAll() \\\n",
    " .execute()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
