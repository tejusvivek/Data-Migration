{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e28e548d-73fa-4497-b56f-24832283f021",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Service Principal Access to ADLS"
    }
   },
   "outputs": [],
   "source": [
    "dbutils.secrets.listScopes()\n",
    "dbutils.secrets.list(\"adls-key\")\n",
    "service_credential = dbutils.secrets.get(scope=\"adls-key\",key=\"migrationAppAuth\")\n",
    "application_id = \"5403bb6e-a9d0-44a6-a025-de1ebc2a7881\"\n",
    "directory_id = \"4249dcf4-f4a1-44f9-940d-14b50a777dd8\"\n",
    "\n",
    "spark.conf.set(\"fs.azure.account.auth.type.adls21s.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(\"fs.azure.account.oauth.provider.type.adls21s.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.id.adls21s.dfs.core.windows.net\", application_id)\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.secret.adls21s.dfs.core.windows.net\", service_credential)\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.endpoint.adls21s.dfs.core.windows.net\", \"https://login.microsoftonline.com/{}/oauth2/token\".format(directory_id))\n",
    "\n",
    "display(dbutils.fs.ls(\"abfss://raw@adls21s.dfs.core.windows.net\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65aa51bf-37a4-42f8-ade1-61358a93aab6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Mount All Containers"
    }
   },
   "outputs": [],
   "source": [
    "configs = {\"fs.azure.account.auth.type\": \"OAuth\",\n",
    "          \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "          \"fs.azure.account.oauth2.client.id\": application_id,\n",
    "          \"fs.azure.account.oauth2.client.secret\": service_credential,\n",
    "          \"fs.azure.account.oauth2.client.endpoint\": f\"https://login.microsoftonline.com/{directory_id}/oauth2/token\"}\n",
    "\n",
    "mountsDict = {\"/mnt/adls21s/raw/\" : \"abfss://raw@adls21s.dfs.core.windows.net/\",\n",
    "              \"/mnt/schema/\" : \"abfss://schema@adls21s.dfs.core.windows.net/\",\n",
    "              \"/mnt/adls21s/bronze/\" : \"abfss://bronze@adls21s.dfs.core.windows.net/\",\n",
    "              \"/mnt/adls21s/silver/\" : \"abfss://silver@adls21s.dfs.core.windows.net/\",\n",
    "              \"/mnt/adls21s/gold/\" : \"abfss://gold@adls21s.dfs.core.windows.net/\"}\n",
    "\n",
    "def checkIfMounted(mounts):\n",
    "    if(any(mount.mountPoint == \"/mnt/adls21s/raw/\" for mount in mounts )):\n",
    "        print(\"Mount point exists, Unmounting...\")\n",
    "        dbutils.fs.unmount(\"/mnt/adls21s/raw/\")\n",
    "        print(\"Unmounted Successfully\")\n",
    "        dbutils.fs.mount(source = \"abfss://raw@adls21s.dfs.core.windows.net/\", mount_point = \"/mnt/adls21s/raw/\", extra_configs = configs)\n",
    "        print(\"Mounted Successfully\")\n",
    "        display(dbutils.fs.mounts())\n",
    "    else:\n",
    "        print(\"Mount point does not exist\")\n",
    "        dbutils.fs.mount(source = \"abfss://raw@adls21s.dfs.core.windows.net/\", mount_point = \"/mnt/adls21s/raw/\", extra_configs = configs)\n",
    "        print(\"Mounted Successfully\")\n",
    "\n",
    "for mountPoint in mountsDict:\n",
    "    try:\n",
    "        dbutils.fs.mount(source = mountsDict[mountPoint], mount_point = mountPoint, extra_configs = configs)\n",
    "        print('No existing mount point found, Mounting {}'.format(mountPoint))\n",
    "    except Exception as e:\n",
    "        #checkIfMounted(mountsList)\n",
    "        print(\"Mount point {} exists, skipping...\".format(mountPoint))\n",
    "display(dbutils.fs.mounts())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5be73d97-cd04-46ce-a0bc-4116f993cc96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, DateType, DoubleType, ShortType,TimestampNTZType\n",
    "\n",
    "cardsDataSchema = StructType([StructField(\"id\", IntegerType(), True), \n",
    "                              StructField(\"client_id\", IntegerType(), True), \n",
    "                              StructField(\"card_brand\", StringType(), True), \n",
    "                              StructField(\"card_type\", StringType(), True), \n",
    "                              StructField(\"card_number\", LongType(), True), \n",
    "                              StructField(\"expires\", StringType(), True), \n",
    "                              StructField(\"cvv\", ShortType(), True), \n",
    "                              StructField(\"has_chip\", StringType(), True), \n",
    "                              StructField(\"num_cards_issued\", ShortType(), True), \n",
    "                              StructField(\"credit_limit\", StringType(), True), \n",
    "                              StructField(\"acct_open_date\", StringType(), True), \n",
    "                              StructField(\"year_pin_last_changed\", IntegerType(), True), \n",
    "                              StructField(\"card_on_dark_web\", StringType(), True)])\n",
    "\n",
    "\n",
    "raw_cards_data_df = spark.read.format(\"csv\").option(\"header\", \"true\").schema(cardsDataSchema)\\\n",
    "    .load(\"/mnt/adls21s/raw/cards_data.csv\")\n",
    "display(raw_cards_data_df)\n",
    "\n",
    "tranactionsDataSchema = StructType([StructField(\"id\", IntegerType(), True),\n",
    "                                    StructField(\"date\", TimestampNTZType(), True),\n",
    "                                    StructField(\"client_id\", IntegerType(), True),\n",
    "                                    StructField(\"card_id\", IntegerType(), True),\n",
    "                                    StructField(\"amount\", StringType(), True),\n",
    "                                    StructField(\"use_chip\", StringType(), True),\n",
    "                                    StructField(\"merchant_id\", IntegerType(), True),\n",
    "                                    StructField(\"merchant_city\", StringType(), True),\n",
    "                                    StructField(\"merchant_state\", StringType(), True),\n",
    "                                    StructField(\"zip\", StringType(), True),\n",
    "                                    StructField(\"mcc\", IntegerType(), True),\n",
    "                                    StructField(\"errors\", StringType(), True)])\n",
    "\n",
    "raw_transactions_data_df = spark.read.format(\"csv\").option(\"header\", \"true\").schema(tranactionsDataSchema)\\\n",
    "    .load(\"/mnt/adls21s/raw/transactions_data.csv\")\n",
    "raw_transactions_data_df.show()\n",
    "\n",
    "usersDataSchema = StructType([StructField(\"id\", IntegerType(), True),\n",
    "                              StructField(\"current_age\", IntegerType(), True),\n",
    "                              StructField(\"retirement_age\", IntegerType(), True),\n",
    "                              StructField(\"birth_year\", IntegerType(), True),\n",
    "                              StructField(\"birth_month\", IntegerType(), True),\n",
    "                              StructField(\"gender\", StringType(), True),\n",
    "                              StructField(\"address\", StringType(), True),\n",
    "                              StructField(\"latitude\", DoubleType(), True),\n",
    "                              StructField(\"longitude\", DoubleType(), True),\n",
    "                              StructField(\"per_capita_income\", StringType(), True),\n",
    "                              StructField(\"yearly_income\", StringType(), True),\n",
    "                              StructField(\"total_debt\", StringType(), True),\n",
    "                              StructField(\"credit_score\", IntegerType(), True),\n",
    "                              StructField(\"num_credit_cards\", IntegerType(), True)])\n",
    "                              \n",
    "raw_users_data_df = spark.read.format(\"csv\").option(\"header\", \"true\").schema(usersDataSchema)\\\n",
    "    .load(\"/mnt/adls21s/raw/users_data.csv\")\n",
    "display(raw_users_data_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bba6f00-6449-47e6-97a9-2dbd65f5dc46",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Adding timestamp and  Removing NULL values from dataframe"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp, regexp_replace, col, count, when, length, year\n",
    "\n",
    "'''Add timestamp column to dataframe'''\n",
    "raw_cards_data_df = raw_cards_data_df.withColumn(\"ingested_at\", current_timestamp())\n",
    "raw_transactions_data_df = raw_transactions_data_df.withColumn(\"ingested_at\", current_timestamp())\n",
    "raw_users_data_df = raw_users_data_df.withColumn(\"ingested_at\", current_timestamp())\n",
    "\n",
    "'''Checking for invalid values'''\n",
    "raw_users_data_df.filter(\"current_age < 0\" or \"retirement_age < 0\" or \"birth_year < 0\" or \"birth_month < 0\" or \"credit_score < 0\" or \"current_age>100\" or \"credit_score > 900\").show()\n",
    "\n",
    "raw_cards_data_df.filter(length(\"card_number\") != 16).show()\n",
    "\n",
    "raw_users_data_df.filter((year(\"ingested_at\") - \"birth_year\") > 115).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "619edddf-48cb-431e-b3f8-c505a149bfb0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Writing data to Bronze stage"
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Path to Delta table\n",
    "users_data_delta_table_path = \"/mnt/adls21s/bronze/users_data/\"\n",
    "cards_data_delta_table_path = \"/mnt/adls21s/bronze/cards_data/\"\n",
    "transactions_data_delta_table_path = \"/mnt/adls21s/bronze/transactions_data/\"\n",
    "\n",
    "# Create Delta table (if not already created)\n",
    "if not DeltaTable.isDeltaTable(spark, users_data_delta_table_path):\n",
    "    raw_users_data_df.write.format(\"delta\").mode(\"overwrite\").save(users_data_delta_table_path)\n",
    "    if not DeltaTable.isDeltaTable(spark,cards_data_delta_table_path):\n",
    "        raw_cards_data_df.write.format(\"delta\").mode(\"overwrite\").save(cards_data_delta_table_path)\n",
    "        if not DeltaTable.isDeltaTable(spark, transactions_data_delta_table_path):\n",
    "            raw_transactions_data_df.write.format(\"delta\").mode(\"overwrite\").save(transactions_data_delta_table_path)\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "# Load the existing Delta table\n",
    "users_data_delta_table = DeltaTable.forPath(spark, (users_data_delta_table_path))\n",
    "cards_data_delta_table = DeltaTable.forPath(spark,(cards_data_delta_table_path))\n",
    "transactions_data_delta_table = DeltaTable.forPath(spark,(transactions_data_delta_table_path))\n",
    "\n",
    "# Define the condition for the merge: match based on a unique key\n",
    "merge_condition = \"t1.id = t2.id\"\n",
    "\n",
    "# Perform the merge (upsert operation)\n",
    "users_data_delta_table.alias(\"t1\").merge(\n",
    "    raw_users_data_df.alias(\"t2\"),\n",
    "    merge_condition\n",
    ").whenMatchedUpdateAll() \\\n",
    " .whenNotMatchedInsertAll() \\\n",
    " .execute()\n",
    "\n",
    "cards_data_delta_table.alias(\"t1\").merge(\n",
    "    raw_cards_data_df.alias(\"t2\"),\n",
    "    merge_condition\n",
    ").whenMatchedUpdateAll() \\\n",
    " .whenNotMatchedInsertAll() \\\n",
    " .execute()\n",
    "\n",
    "transactions_data_delta_table.alias(\"t1\").merge(\n",
    "    raw_transactions_data_df.alias(\"t2\"),\n",
    "    merge_condition\n",
    ").whenMatchedUpdateAll() \\\n",
    " .whenNotMatchedInsertAll() \\\n",
    " .execute()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
